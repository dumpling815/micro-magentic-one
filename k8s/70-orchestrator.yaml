apiVersion: apps/v1
kind: Deployment
metadata:
  name: orchestrator
  namespace: micro-magentic-one
  labels: { app: orchestrator, project: micro-magentic-one }
spec:
  replicas: 1
  selector:
    matchLabels: { app: orchestrator }
  template:
    metadata:
      labels: { app: orchestrator, project: micro-magentic-one }
    spec:
      # (선택) GPU 노드 선호
      # affinity:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #         - matchExpressions:
      #             - key: accelerator
      #               operator: In
      #               values: ["gpu"]

      # If using OpenAI API or else(not local model), comment this out
      initContainers:
        - name: ollama-sidecar
          image: ollama/ollama:latest
          command: ["bash","-lc"]
          args:
            - |
              ollama serve & sleep 2
              ollama pull llama3.2:1b
          # volumeMounts:
          #   - { name: ollama-models, mountPath: /root/.ollama }

      containers:
        - name: orchestrator
          image: docker.io/minuk0815/micro-magentic-one-orchestrator:0.0.4 # Tag version should be correct.
          imagePullPolicy: IfNotPresent
          ports: [ { containerPort: 8080 } ]
          envFrom: [ { configMapRef: { name: micro-magentic-one-orch-config } } ]
          # volumeMounts:
          #   - { name: shared-data,   mountPath: /data }
          readinessProbe:
            httpGet: { path: /health, port: 8080 }
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            requests: { cpu: "1", memory: "8Gi" }
            limits:
              cpu: "4"
              memory: "16Gi"

        - name: ollama
          image: ollama/ollama:latest
          imagePullPolicy: IfNotPresent
          ports: [ { containerPort: 11434 } ]
          env:
            - { name: OLLAMA_KEEP_ALIVE, value: "5m" }
            - { name: OLLAMA_NUM_PARALLEL, value: "1" }
          # volumeMounts:
          #   - { name: ollama-models, mountPath: /root/.ollama }
          readinessProbe:
            tcpSocket: { port: 11434 }
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            requests: { cpu: "1", memory: "8Gi" }
            limits:
              nvidia.com/gpu: 1
              cpu: "4"
              memory: "16Gi"

      # volumes:
      #   - name: shared-data
      #     persistentVolumeClaim: { claimName: micro-magentic-one-pvc }
      #   - name: ollama-models
      #     persistentVolumeClaim: { claimName: ollama-models-pvc }
---
apiVersion: v1
kind: Service
metadata:
  name: orchestrator
  namespace: micro-magentic-one
  labels: { app: orchestrator, project: micro-magentic-one }
spec:
  selector: { app: orchestrator }
  ports:
    - { name: http, port: 8080, targetPort: 8080, protocol: TCP }
  type: ClusterIP