# =========================
# CODER (GPU 사용)
# =========================
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coder
  namespace: micro-magentic-one
  labels: { app: coder, project: micro-magentic-one }
spec:
  replicas: 1
  selector:
    matchLabels: { app: coder }
  template:
    metadata:
      labels: { app: coder, project: micro-magentic-one }
    spec:
      # (선택) GPU 노드 선호
      # affinity:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #         - matchExpressions:
      #             - key: accelerator
      #               operator: In
      #               values: ["gpu"]
      runtimeClassName: nvidia # GPU 사용 시 필요
      initContainers:
        - name: ollama-prefetch
          image: ollama/ollama:latest
          imagePullPolicy: IfNotPresent
          envFrom:
            - configMapRef: { name: micro-magentic-one-config }
          command: ["bash","-lc"]
          args:       # Default 모델은 llama3.2:1b -> ${VARIABLE:-default} 형태로 사용 가능.
            - |
              ollama serve & sleep 2
              MODELS="${OLLAMA_PREFETCH_MODELS:-llama3.2:1b}"
              for m in $MODELS; do
                echo "Prefetching model: $m"
                ollama pull "$m" || { echo "Failed to pull $m"; exit 1; }
              done
          volumeMounts:
            - { name: ollama-models, mountPath: /root/.ollama }

      containers:
        - name: coder
          image: docker.io/minuk0815/micro-magentic-one-coder:0.0.20
          imagePullPolicy: IfNotPresent
          ports: [ { containerPort: 8080 } ]
          envFrom:
            - configMapRef: { name: micro-magentic-one-config }
          volumeMounts:
            - { name: shared-data, mountPath: /data }
          readinessProbe:
            httpGet: { path: /health, port: 8080 }
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            requests: { cpu: "1", memory: "2Gi" }
            limits:
              cpu: "2"
              memory: "4Gi"

        - name: ollama
          image: ollama/ollama:latest
          imagePullPolicy: IfNotPresent
          ports: [ { containerPort: 11434 } ]
          envFrom:
            - configMapRef: { name: micro-magentic-one-config }
          volumeMounts:
            - { name: ollama-models, mountPath: /root/.ollama }
          readinessProbe:
            tcpSocket: { port: 11434 }
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            requests: { cpu: "1", memory: "2Gi" }
            limits:
              nvidia.com/gpu: 1
              cpu: "2"
              memory: "4Gi"

      volumes:
        - { name: shared-data,   persistentVolumeClaim: { claimName: micro-magentic-one-pvc } }
        - { name: ollama-models, persistentVolumeClaim: { claimName: ollama-models-pvc } }
---
apiVersion: v1
kind: Service
metadata:
  name: coder
  namespace: micro-magentic-one
  labels: { app: coder, project: micro-magentic-one }
spec:
  selector: { app: coder }
  ports:
    - { name: http, port: 8080, targetPort: 8080, protocol: TCP }
  type: ClusterIP